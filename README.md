# ObservatÃ³rio de CT&I sobre Plantas EndÃªmicas da AmazÃ´nia

Este repositÃ³rio contÃ©m o cÃ³digo-fonte e a metodologia para o pipeline de dados desenvolvido como parte da dissertaÃ§Ã£o [SEU TÃTULO AQUI]. O objetivo deste projeto Ã© automatizar a coleta, limpeza, modelagem e integraÃ§Ã£o de dados de mÃºltiplas fontes para a criaÃ§Ã£o de um painel de indicadores de CiÃªncia, Tecnologia e InovaÃ§Ã£o (CT&I) sobre a flora endÃªmica da AmazÃ´nia.

Este trabalho representa a **Fase 1** do projeto, focada na construÃ§Ã£o da infraestrutura de dados.

---

## ğŸ›ï¸ Arquitetura do Pipeline

O projeto implementa um pipeline de ExtraÃ§Ã£o, TransformaÃ§Ã£o e Carga (ETL) que processa dados brutos e os estrutura em um **Modelo Estrela HÃ­brido**, otimizado para anÃ¡lise em ferramentas de Business Intelligence (BI) como o Looker Studio.

O pipeline Ã© composto por:
* **Scripts de Processamento por Fonte:** MÃ³dulos dedicados para `Scopus`, `CNCFlora` e `Espacenet`.
* **Script de UnificaÃ§Ã£o:** ResponsÃ¡vel por criar dimensÃµes conformes (mestras), como a `dim_especies_mestre`, para conectar as diferentes fontes.
* **Script Orquestrador (`main.py`):** Gerencia a execuÃ§Ã£o de todo o pipeline em um Ãºnico comando, respeitando as dependÃªncias de dados.

## ğŸ› ï¸ Tecnologias Utilizadas

* **Linguagem:** Python 3.10+
* **Bibliotecas Principais:** Pandas
* **Ambiente:** Visual Studio Code (Codespaces)
* **Controle de VersÃ£o:** Git & GitHub

## ğŸš€ Guia de InstalaÃ§Ã£o e ExecuÃ§Ã£o

Siga os passos abaixo para replicar o ambiente e executar o pipeline de dados.

### 1. ConfiguraÃ§Ã£o do Ambiente

1.  **Clonar o RepositÃ³rio:**
    ```bash
    git clone <URL_DO_SEU_REPOSITORIO_GITHUB>
    cd <NOME_DA_PASTA_DO_PROJETO>
    ```

2.  **Criar e Ativar o Ambiente Virtual:**
    ```bash
    # Criar o ambiente
    python3 -m venv .venv
    
    # Ativar no macOS/Linux
    source .venv/bin/activate
    
    # Ativar no Windows
    # .venv\Scripts\activate
    ```

3.  **Instalar as DependÃªncias:**
    Certifique-se de que o arquivo `requirements.txt` exista com o conteÃºdo abaixo e execute o comando de instalaÃ§Ã£o.

    *ConteÃºdo do `requirements.txt`:*
    ```
    pandas
    ```

    *Comando de InstalaÃ§Ã£o:*
    ```bash
    pip install -r requirements.txt
    ```

### 2. OrganizaÃ§Ã£o dos Dados Brutos

Antes da execuÃ§Ã£o, os arquivos de dados brutos devem ser posicionados na seguinte estrutura dentro de `data/raw/`:

data/raw/
â”œâ”€â”€ cncflora/
â”‚   â”œâ”€â”€ lista_vermelha_cnc_flora.csv
â”‚   â””â”€â”€ termos_plantas.txt
â”œâ”€â”€ espacenet_input/
â”‚   â”œâ”€â”€ espacenet_angiosperma.csv
â”‚   â””â”€â”€ espacenet_samambaias_e_licofitas.csv
â”œâ”€â”€ scopus_input/
â”‚   â”œâ”€â”€ scopus_angiospermas.csv
â”‚   â””â”€â”€ ... (outros arquivos da Scopus)
â””â”€â”€ espacenet_resumo_plantas.csv

### 3. ExecuÃ§Ã£o do Pipeline Completo

Com o ambiente configurado e os dados no lugar, execute o pipeline com um Ãºnico comando a partir da **pasta raiz** do projeto:

```bash
python main.py

O script irÃ¡ orquestrar todos os passos, exibindo o progresso no terminal. Ao final, todos os arquivos processados e modelados estarÃ£o na pasta data/processed/.

âš™ï¸ DescriÃ§Ã£o dos Componentes do Pipeline
scripts/processar_scopus.py: Limpa, transforma e modela os dados da Scopus.

scripts/processar_cncflora.py: Processa os dados da CNCFlora.

scripts/processar_espacenet.py: Processa os dados de patentes da Espacenet.

scripts/unificar_fontes.py: Integra os outputs dos scripts de processamento, criando as dimensÃµes mestras.

main.py: Orquestrador principal que executa todos os outros scripts na ordem correta.

ğŸ—ºï¸ Roadmap de Trabalhos Futuros
Fase 2: InclusÃ£o de novas fontes de dados sobre sustentabilidade e unificaÃ§Ã£o com o modelo atual.

Fase 3: ConexÃ£o do Data Warehouse final com o Looker Studio, desenvolvimento dos indicadores visuais e avaliaÃ§Ã£o geral do processo.
